{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Amazon_Movie_Parser as amp\n",
    "import Graph_Amazon_Movies as gam\n",
    "# save numpy array as csv file\n",
    "from numpy import savetxt\n",
    "# load numpy array from csv file\n",
    "from numpy import loadtxt\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import Computations_debug as cmp\n",
    "from networkx.algorithms import bipartite\n",
    "import random\n",
    "import Predict_Movie_v1 as pm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pylot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization: TILE (broadcast) or MATRIX (np.newaxis) (FASTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization test with TILE\n",
    "tt = np.array([[2, 4, 6], [1, 5, 8], [20, 4, 6]])\n",
    "#print(tt.mean(axis=1))\n",
    "#tt_norm=tt/tt.mean(axis=1)\n",
    "\n",
    "print('Sample matrix:\\n{}'.format(tt))\n",
    "\n",
    "tt_sum = tt.sum(axis = 1)\n",
    "print('\\nSum of each row:\\n{}'.format(tt.sum(axis=1)))\n",
    "\n",
    "# broadcast with TILE so that we can divide the each element in the row with the same number \n",
    "# we will do an element wise division not a dot product here\n",
    "tt_sum_broadcasted = np.tile(tt_sum, (3, 1)).T\n",
    "print('\\nBroadcasting with TILE:\\n{}'.format(tt_sum_broadcasted))\n",
    "\n",
    "# divide each element in the row with the sum of the row so the sum will be equal to 1\n",
    "# in other words, it will give the probability/rating for the each cell\n",
    "tt_norm = tt / tt_sum_broadcasted\n",
    "print('\\nNormalizing with TILE:\\n{}'.format(tt_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are testing the normalization using the prepared function, to see if it works as expected\n",
    "cmp1 = cmp.GraphComp()\n",
    "tt = np.array([[2, 4, 6], [1, 5, 8], [20, 4, 6]])\n",
    "\n",
    "#print('Sample matrix:\\n{}'.format(tt))\n",
    "\n",
    "# TILE put in function Normalize_Array()\n",
    "tt_norm = cmp1.Normalize_Array(tt, axis = 1)\n",
    "print('\\nNormalizing with Normalize_Array function:\\n{}'.format(tt_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are testing the normalization using the prepared function, to see if it works as expected\n",
    "cmp1 = cmp.GraphComp()\n",
    "tt = np.array([[2, 4, 6], [1, 5, 8], [20, 4, 6]])\n",
    "\n",
    "#print('Sample matrix:\\n{}'.format(tt))\n",
    "\n",
    "#Normalize_Matrix(self, arr_to_normalize, dim = 'row'):\n",
    "tt_norm = cmp1.Normalize_Matrix(tt, dim = 'tt')\n",
    "print('\\nNormalizing with Normalize_Matrix function:\\n{}'.format(tt_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Random Numbers: choices vs sample (UNIQUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = [2, 4, 6, 8, 9, 11]\n",
    "# random.choices \n",
    "# random.sample returns unique numbers unlike random.choices \n",
    "random.sample(tt, k = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from Graph to Matrix using Networkx (FASTER than FOR LOOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see normalization from our dataset to verify that it is working fine\n",
    "# below that one, you will find a 3x3 matrix simulation, if it would be easier to understand, you can check that one first..\n",
    "grpComp = cmp.GraphComp()\n",
    "grp = gam.Graph_Amazon()\n",
    "\n",
    "# here, there is no Connected or Disconnected check, so, the graph generated will most likele be a DISCONNECTED graph\n",
    "DISconnected_gr_amazon_movies = grp.Create_Graph(file_name = 'data/movies.txt', n_movies = 8, prs_out = 'dictionary')\n",
    "# here, we set n_movies = 8 but we have 10 nodes. Why is that?\n",
    "# Because 8 actually shows the number of edges, not number of the movies (BAD NAMING!)\n",
    "# So, we have 8 edges and in this 8 edges, we have 10 unique nodes. It might be like 8 users + 2 movies\n",
    "\n",
    "P_disconnected = nx.to_numpy_matrix(DISconnected_gr_amazon_movies)\n",
    "print(type(P_disconnected))\n",
    "\n",
    "print('P_disconnected:\\n{}'.format(P_disconnected))\n",
    "\n",
    "P_disconnected_norm = grpComp.Normalize_Matrix(P_disconnected, dim = 'row')\n",
    "print('\\nP_disconnected-NORMALIZED:\\n{}'.format(P_disconnected_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another simulation with a smaller matrix, it migh\n",
    "tt2 = np.array([[0, 4, 2], [4, 0, 0], [2, 0, 0]])\n",
    "tt2 = np.asmatrix(tt2)\n",
    "print(type(tt2))\n",
    "#tt2_norm = cmp1.Normalize_Matrix(tt2, dim = 'tt2')\n",
    "tt2_norm = cmp1.Normalize_Matrix(tt2, dim = 'row')\n",
    "\n",
    "print('Sample matrix:\\n{}'.format(tt2))\n",
    "print('\\nSample matrix_NORM:\\n{}'.format(tt2_norm))\n",
    "\n",
    "r_tt = np.zeros(tt2_norm.shape[0])\n",
    "r_tt[1] = 1\n",
    "r_tt_zero = r_tt.copy()\n",
    "\n",
    "print('r at the beginning:\\n{}'.format(r_tt_zero))\n",
    "\n",
    "print('\\nLets see how random walk works by selecting the second node_P[1]:'.format(r_tt))\n",
    "#r = (1 - beta) * np.dot(r, P_norm) + beta * r_zero\n",
    "\n",
    "# r vectors can be generated in the function but we started to design in that way, sos didn't change it later..\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('r - after 1st step:\\n{}'.format(r_tt))\n",
    "# as you can see, after the first strep, the value for r[1]=0.15  and the SUM(row) = 1\n",
    "\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 2nd step:\\n{}'.format(r_tt))\n",
    "\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 3rd step:\\n{}'.format(r_tt))\n",
    "\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 4th step:\\n{}'.format(r_tt))\n",
    "\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 5th step:\\n{}'.format(r_tt))\n",
    "\n",
    "# You will easily realize that, for the even number of steps, the probability is high on the user himself,\n",
    "# on the odd number of steps, the probability is high on the movie he reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpComp = cmp.GraphComp()\n",
    "\n",
    "# *** min numner of reviews is 140 \n",
    "# if we want to make a connected bipartite graph & if we start picking from review#1\n",
    "grp = gam.Graph_Amazon()\n",
    "file_name='data/movies.txt'; n_movies = 140; prs_out='dictionary'\n",
    "\n",
    "max_connected_gr_amazon_movies = grpComp.Create_Bipartite_Giant_Component(grp, file_name, n_movies, prs_out)\n",
    "\n",
    "P = nx.to_numpy_matrix(max_connected_gr_amazon_movies)\n",
    "\n",
    "grpComp.Normalize_Matrix(P, dim = 'row')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.zeros(P_disconnected_norm.shape[0])\n",
    "r[1] = 1\n",
    "r_zero = r.copy()\n",
    "\n",
    "print('r at the beginning:\\n{}'.format(r_zero))\n",
    "\n",
    "print('\\nLets see how random walk works by selecting the second node_P[1]:'.format(r))\n",
    "#r = (1 - beta) * np.dot(r, P_norm) + beta * r_zero\n",
    "\n",
    "r = grpComp.random_walk_vector(P_disconnected_norm, r, r_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 1st step:\\n{}'.format(r))\n",
    "# as you can see, after the first strep, the value for r[1]=0.15  and the SUM(row) = 1\n",
    "\n",
    "r = grpComp.random_walk_vector(P_disconnected_norm, r, r_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 2nd step:\\n{}'.format(r))\n",
    "\n",
    "r = grpComp.random_walk_vector(P_disconnected_norm, r, r_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 3rd step:\\n{}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Common items in 2 lists with SET (FASTER) - WITHOUT FOR loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_nodes = ['u1', 'u2', 'u3', 'u4']\n",
    "bottom_nodes_v = ['u_v1', 'u2', 'u_v3', 'u4', 'u_v5']\n",
    "lst_common_users = list(set(bottom_nodes) - (set(bottom_nodes) - set(bottom_nodes_v)))\n",
    "#lst_common_users = list(set(bottom_nodes_v) - (set(bottom_nodes_v) - set(bottom_nodes)))\n",
    "print(lst_common_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph from LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test = [('A141HP4LYPW', 'B003AI2VGA', '3.0'),\n",
    "     ('B000063W1R', 'A141HP4LYPW', '4.0'),\n",
    "     ('6304286961', 'A141HP4LYPW', '5.0'),\n",
    "     ('A141HP4LYPW', '5556167281', '5.0')]\n",
    "\n",
    "grp1 = gam.Graph_Amazon()\n",
    "grp_test=grp1.Create_Graph_From_List_WITH_Weight(lst_test)\n",
    "\n",
    "print(grp_test.nodes)\n",
    "\n",
    "grp_test_martrix = nx.to_numpy_matrix(grp_test)\n",
    "\n",
    "print(grp_test_martrix)\n",
    "# as uyou can see, we have 1 user 4 movies in this set (which is unusual, movies are usually less)\n",
    "# but this is just an artificial set - you can easily play and test some functions if you would like to now\n",
    "# i.e. Normalization, Random walk, check networkx functions, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_test.edges.data('weight', default=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_test.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(grp_test.nodes)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_test.has_edge('A141HP4LYPW', 'B003AI2VGAx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdata = grp_test.get_edge_data('A141HP4LYPW', 'B003AI2VGA')\n",
    "print(gdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdata['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_test.get_edge_data('A141HP4LYPW', 'B003AI2VGA')['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grp_test.get_edge_data('A141HP4LYPW', 'B003AI2VGAx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WAS AN INTERNAL TEST - just keeping as a side note..\n",
    "# by doing the below conversion, we are just considering the links between the nodes, we ignore the weights\n",
    "# we try to understand if considering the weights will benefit us or not\n",
    "#P[P>0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(llt)[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nodes=np.argsort(r)\n",
    "\n",
    "user_similarity = []\n",
    "\n",
    "# we will not consider the user himself or movies as similar nodes\n",
    "for idx in sorted_nodes[0][0]:\n",
    "    if idx != 3 and idx > 0:\n",
    "        user_similarity.append(idx)\n",
    "        \n",
    "user_similarity_top2 = user_similarity[-2:]\n",
    "\n",
    "print(user_similarity)\n",
    "print(user_similarity_top2)\n",
    "print(list(grp_test.nodes)[user_similarity_top2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Edges from movies file, OUTPUT: Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code was quickly written so, it might not be necessarily starting from the 4th review but maybe 5th review\n",
    "# sensitive calibration haven't done, so index can be +1. \n",
    "# but in any case, it gurantees to avoid the overlap (if unlucky, max 1 overlap will be in place)\n",
    "file_name = 'data/movies.txt'; start_index = 4; n_movies = 5; prs_out = 'screen'\n",
    "\n",
    "#movie_dict_old = amp.AmazonMovies(n_movies, file_name, prs_out)\n",
    "movie_dict_old = amp.AmazonMovies_VALIDATION(n_movies, file_name, prs_out, start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = 'data/movies.txt'; start_index = 2; n_movies = 10; prs_out = 'screen'\n",
    "\n",
    "movie_dict = amp.AmazonMovies_VALIDATION(n_movies, file_name, prs_out, start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH in movies.txt for specific items (FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the search uses 'like' criteria so it search for any string mathcing \n",
    "# not a perfect search, just used to debug something -can definitely be improved..\n",
    "import Amazon_Movie_Parser as amp\n",
    "file_name = 'data/movies.txt'; n_movies=60000; prs_out = 'screen'\n",
    "userID = 'AR15V2ULA2EYM';   movieID = ''  #movieID = 'B00005952Q'\n",
    "\n",
    "amp.Filter_AmazonMovies(n_movies, userID, movieID, file_name, prs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_true = [0, 0, 0, 0, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 0, 1]\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "lst_tpr = list(tpr); lst_fpr = list(fpr); lst_thresholds = list(thresholds)\n",
    "\n",
    "print(fpr)\n",
    "print(tpr)\n",
    "print(thresholds)\n",
    "\n",
    "len_tpr = len(lst_tpr)\n",
    "lst_precision = np.zeros(len_tpr)\n",
    "lst_recall = np.zeros(len_tpr)\n",
    "        \n",
    "for i in range(len(lst_tpr)):\n",
    "    lst_precision[i] = lst_tpr[i]/(lst_tpr[i] + lst_fpr[i])\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average='macro') # 'micro'  'weighted' None  average=None, zero_division=1\n",
    "print(precision)\n",
    "\n",
    "precision_recall = precision_recall_curve(y_true, y_pred)\n",
    "print(list(precision_recall))\n",
    "\n",
    "avg_precision = average_precision_score(y_true, y_pred)\n",
    "print(avg_precision)\n",
    "\n",
    "print(lst_sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1=[1,2,3]\n",
    "lst2=[3,5,1]\n",
    "lst3=[8,2,4]\n",
    "lstr = [lst1, lst2, lst3]\n",
    "lstr_2 = []\n",
    "\n",
    "max_i=0\n",
    "max_i_val=0\n",
    "for i, l in enumerate(lstr):\n",
    "    lstr_2.append(l[2])\n",
    "    if l[0] > max_i_val:\n",
    "        max_i_val = l[0]\n",
    "        max_i = i\n",
    "\n",
    "print(max_i)\n",
    "print(max_i_val)\n",
    "print(lstr[max_i])\n",
    "print(lstr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst4=[8,2,4]\n",
    "print(lst4)\n",
    "lst4.remove(8)\n",
    "print(lst4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Test to Check if any RUN-time Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** 1st Graph calculations - MODEL *************** \n",
      "Calculation time-Crete graph from the movies file: 0:00:00.579693\n",
      "Is connected: True ... Is bipartite: True\n",
      "BEFORE: # of nodes: 4408 ... # of edges: 4643\n",
      "Calculation time-1st Part: 0:00:00.742393\n",
      "\n",
      "*************** 2nd Graph calculations - TEST *************** \n",
      "Create_Bipartite_VALIDATION is running...\n",
      "Calculation time-Crete graph from the movies file: 0:00:00.248519\n",
      "Is connected: True ... Is bipartite: True\n",
      "BEFORE: # of nodes: 2487 ... # of edges: 2585\n",
      "n_reviews for modeling (P matrix) = 61 .. n_movies for testing = 65\n",
      "n_users for modeling (P matrix) = 4347 .. n_users for testing = 2422\n",
      "Calculation time-2nd Part: 0:00:00.348293\n",
      "Preparation starts at 2020-05-23 14:23:42.612856\n",
      "P to_numpy_matrix: 2020-05-23 14:23:42.725729\n",
      "P normalize: 2020-05-23 14:23:42.869005\n",
      "P finalize: 2020-05-23 14:23:42.973682\n",
      "Calculation time-P matrix generation from the graph: 0:00:00.247952\n",
      "P_norm is generated: 2020-05-23 14:23:42.986122\n",
      "Calculation time-user-movie indexes: 0:00:00.161295\n",
      "\n",
      "\n",
      "Calculation time-user-movie indexes: 0:00:00.063444\n",
      "\n",
      "\n",
      "FOR loop starts in top-K users: 2020-05-23 14:23:43.221326\n",
      "0. of 20 Random walk starts for 3960-A3R2YB0WTTB0IJ at 2020-05-23 14:23:43.222175\n",
      "Calculate the ratios per movie: 0\n",
      "10. of 20 Random walk starts for 147-ANCOMAI0I7LVG at 2020-05-23 14:23:47.359891\n",
      "Calculate the ratios per movie: 10\n",
      "19. of 20 Random walk starts for 1608-A3FEQXNCFC68KC at 2020-05-23 14:23:50.853399\n",
      "Calculate the ratios per movie: 19\n",
      "MOVIES >> YES: Exist in both .. NO: Exist in dataset-2 but not exist in dataset-1\n",
      "count_YES: 0 .. count_NO: 65\n",
      "USERS >> YES: Exist in both .. NO: Exist in dataset-2 but not exist in dataset-1\n",
      "count_YES: 198 .. count_NO: 2224\n",
      "MOVIES >> YES: Exist in both .. NO: Exist in dataset-1 but not exist in dataset-2\n",
      "count_YES: 0 .. count_NO: 61\n",
      "USERS >> YES: Exist in both .. NO: Exist in dataset-1 but not exist in dataset-2\n",
      "count_YES: 198 .. count_NO: 4149\n",
      "**********************************************\n",
      "Calculation time for ALL-Predictions: 0:00:08.637852\n",
      "[0.8537234042553191, '', '', '', '', '', '', '', '', '', '', 'decision_4']\n",
      "[0.0, 0.16666666666666666, 1.0, 1.0, '', '', '', '', '', '', '', 0.18181818181818182]\n",
      "[0, 0, 1.0, 0.153846, '', '', '', '', '', '', '', 0.06]\n",
      "[0.0, 0.0, 0.35106382978723405, 1.0, '', '', '', '', '', '', '', '']\n",
      "[21.0, 20.0, 10.0, 0.0, '', '', '', '', '', '', '', '']\n",
      "['Graph Creation', datetime.timedelta(seconds=9, microseconds=739144), 'Random Walk & Prediction', datetime.timedelta(seconds=9, microseconds=739144), 'Threshold search', datetime.timedelta(microseconds=158270), 'TOTAL TIME', datetime.timedelta(seconds=19, microseconds=636558), '', '', '', '']\n",
      "['debug_mode', 'n_movies_model', 'n_movies_validation', 'walk_steps', 'beta', 'top_neighbor', 'n_test_users', 'threshold', 'start_index_v', 'file_name', 'top_N_suggestions', '']\n",
      "['Off', 5000, 3000, 2, 0.15, 10, 20, 10.0, 5000, 'data/movies.txt', 5, '']\n",
      "['threshold', 'acc', 'tpr', 'fpr', 'tnr', 'fnr', 'precision', 'f1', '', '', '', '']\n",
      "[10.0, 0.95, 0.16666666666666666, 0.0, 1.0, 0.8333333333333334, 1.0, 0.2857142857142857, '', '', '', '']\n",
      "Performance Values are saved in RESULTS/PerformanceValues_0.8537_23.5.2020_14.23.51.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>TPR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Thresholds</th>\n",
       "      <th>Calc_Time</th>\n",
       "      <th>lst_name</th>\n",
       "      <th>lst_value</th>\n",
       "      <th>other_metrics</th>\n",
       "      <th>metrics_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.853723</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Graph Creation</td>\n",
       "      <td>debug_mode</td>\n",
       "      <td>Off</td>\n",
       "      <td>threshold</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0:00:09.739144</td>\n",
       "      <td>n_movies_model</td>\n",
       "      <td>5000</td>\n",
       "      <td>acc</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>10</td>\n",
       "      <td>Random Walk &amp; Prediction</td>\n",
       "      <td>n_movies_validation</td>\n",
       "      <td>3000</td>\n",
       "      <td>tpr</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0:00:09.739144</td>\n",
       "      <td>walk_steps</td>\n",
       "      <td>2</td>\n",
       "      <td>fpr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Threshold search</td>\n",
       "      <td>beta</td>\n",
       "      <td>0.15</td>\n",
       "      <td>tnr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0:00:00.158270</td>\n",
       "      <td>top_neighbor</td>\n",
       "      <td>10</td>\n",
       "      <td>fnr</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>TOTAL TIME</td>\n",
       "      <td>n_test_users</td>\n",
       "      <td>20</td>\n",
       "      <td>precision</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0:00:19.636558</td>\n",
       "      <td>threshold</td>\n",
       "      <td>10</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>start_index_v</td>\n",
       "      <td>5000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>file_name</td>\n",
       "      <td>data/movies.txt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>top_N_suggestions</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>decision_4</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.06</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           AUC       TPR Precision       FPR Thresholds  \\\n",
       "0     0.853723         0         0         0         21   \n",
       "1               0.166667         0         0         20   \n",
       "2                      1         1  0.351064         10   \n",
       "3                      1  0.153846         1          0   \n",
       "4                                                         \n",
       "5                                                         \n",
       "6                                                         \n",
       "7                                                         \n",
       "8                                                         \n",
       "9                                                         \n",
       "10                                                        \n",
       "11  decision_4  0.181818      0.06                        \n",
       "\n",
       "                   Calc_Time             lst_name        lst_value  \\\n",
       "0             Graph Creation           debug_mode              Off   \n",
       "1             0:00:09.739144       n_movies_model             5000   \n",
       "2   Random Walk & Prediction  n_movies_validation             3000   \n",
       "3             0:00:09.739144           walk_steps                2   \n",
       "4           Threshold search                 beta             0.15   \n",
       "5             0:00:00.158270         top_neighbor               10   \n",
       "6                 TOTAL TIME         n_test_users               20   \n",
       "7             0:00:19.636558            threshold               10   \n",
       "8                                   start_index_v             5000   \n",
       "9                                       file_name  data/movies.txt   \n",
       "10                              top_N_suggestions                5   \n",
       "11                                                                   \n",
       "\n",
       "   other_metrics metrics_val  \n",
       "0      threshold          10  \n",
       "1            acc        0.95  \n",
       "2            tpr    0.166667  \n",
       "3            fpr           0  \n",
       "4            tnr           1  \n",
       "5            fnr    0.833333  \n",
       "6      precision           1  \n",
       "7             f1    0.285714  \n",
       "8                             \n",
       "9                             \n",
       "10                            \n",
       "11                            "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'data/movies.txt'; n_movies = 5000; n_movies_v = 3000; n_test_users = 20; walk_steps = 2\n",
    "start_index_v = n_movies; beta = 0.15; top_neighbor = 10; top_N_movie_suggestions = 5\n",
    "\n",
    "predict_amazon = pm.Amazon()\n",
    "strt=datetime.datetime.now()\n",
    "max_connected_gr_amazon_movies, max_connected_gr_amazon_movies_VAL = \\\n",
    "    predict_amazon.create_graphs(file_name, n_movies, n_movies_v, start_index_v)\n",
    "\n",
    "df_FINAL = \\\n",
    "    predict_amazon.run_validation_TOP_N(max_connected_gr_amazon_movies, max_connected_gr_amazon_movies_VAL,\\\n",
    "                                  walk_steps, n_test_users, beta, top_neighbor, top_N_movie_suggestions)\n",
    "end1=datetime.datetime.now()\n",
    "diff=end1-strt\n",
    "df_perf = predict_amazon.save_performance_values(df_FINAL, diff, diff)\n",
    "\n",
    "params_df = predict_amazon.show_parameters_in_use()\n",
    "params_df.style.set_properties(**{'text-align': 'left'})\n",
    "\n",
    "df_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N_suggestions=10\n",
    "predict_amazon.df_summary_ratio_v.sort_values(by='ratio_similar', ascending=False).head(top_N_suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'data/movies.txt'; num_reviews = 5\n",
    "amp.AmazonMovies(num_reviews, file_name, outputTo='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_tst = amp.Read_Connected_Movies('Movies_22.5.2020_20.47.56.txt')\n",
    "print(lst_tst)\n",
    "arr_tst = lst_tst[2][0].replace('[','').replace(']','').split(sep=\"' '\")\n",
    "#print(len(lst_tst))\n",
    "print(arr_tst)\n",
    "print(arr_tst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "file_name = 'data/movies.txt'; num_reviews = 75000\n",
    "\n",
    "amp.AmazonMovies(num_reviews, file_name, outputTo='dict_to_file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = amp.Load_Pickle_File_VAL('Movies_22.5.2020_23.21.10__100_reviews.pkl', 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "out = dict(list(xx.items())[2: 7])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1=[1]\n",
    "lst_2=[2,3,1]\n",
    "lst_1+=lst_2\n",
    "print(lst_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1.count(2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Amazon_movies.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
