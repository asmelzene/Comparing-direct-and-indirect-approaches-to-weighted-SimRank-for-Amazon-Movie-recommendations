{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Amazon_Movie_Parser as amp\n",
    "import Graph_Amazon_Movies as gam\n",
    "# save numpy array as csv file\n",
    "from numpy import savetxt\n",
    "# load numpy array from csv file\n",
    "from numpy import loadtxt\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import Computations_debug as cmp\n",
    "from networkx.algorithms import bipartite\n",
    "import random\n",
    "import Predict_Movie_v1 as pm\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pylot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization: TILE (broadcast) or MATRIX (np.newaxis) (FASTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization test with TILE\n",
    "tt = np.array([[2, 4, 6], [1, 5, 8], [20, 4, 6]])\n",
    "#print(tt.mean(axis=1))\n",
    "#tt_norm=tt/tt.mean(axis=1)\n",
    "\n",
    "print('Sample matrix:\\n{}'.format(tt))\n",
    "\n",
    "tt_sum = tt.sum(axis = 1)\n",
    "print('\\nSum of each row:\\n{}'.format(tt.sum(axis=1)))\n",
    "\n",
    "# broadcast with TILE so that we can divide the each element in the row with the same number \n",
    "# we will do an element wise division not a dot product here\n",
    "tt_sum_broadcasted = np.tile(tt_sum, (3, 1)).T\n",
    "print('\\nBroadcasting with TILE:\\n{}'.format(tt_sum_broadcasted))\n",
    "\n",
    "# divide each element in the row with the sum of the row so the sum will be equal to 1\n",
    "# in other words, it will give the probability/rating for the each cell\n",
    "tt_norm = tt / tt_sum_broadcasted\n",
    "print('\\nNormalizing with TILE:\\n{}'.format(tt_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are testing the normalization using the prepared function, to see if it works as expected\n",
    "cmp1 = cmp.GraphComp()\n",
    "tt = np.array([[2, 4, 6], [1, 5, 8], [20, 4, 6]])\n",
    "\n",
    "#print('Sample matrix:\\n{}'.format(tt))\n",
    "\n",
    "# TILE put in function Normalize_Array()\n",
    "tt_norm = cmp1.Normalize_Array(tt, axis = 1)\n",
    "print('\\nNormalizing with Normalize_Array function:\\n{}'.format(tt_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are testing the normalization using the prepared function, to see if it works as expected\n",
    "cmp1 = cmp.GraphComp()\n",
    "tt = np.array([[2, 4, 6], [1, 5, 8], [20, 4, 6]])\n",
    "\n",
    "#print('Sample matrix:\\n{}'.format(tt))\n",
    "\n",
    "#Normalize_Matrix(self, arr_to_normalize, dim = 'row'):\n",
    "tt_norm = cmp1.Normalize_Matrix(tt, dim = 'tt')\n",
    "print('\\nNormalizing with Normalize_Matrix function:\\n{}'.format(tt_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Random Numbers: choices vs sample (UNIQUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = [2, 4, 6, 8, 9, 11]\n",
    "# random.choices \n",
    "# random.sample returns unique numbers unlike random.choices \n",
    "random.sample(tt, k = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from Graph to Matrix using Networkx (FASTER than FOR LOOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see normalization from our dataset to verify that it is working fine\n",
    "# below that one, you will find a 3x3 matrix simulation, if it would be easier to understand, you can check that one first..\n",
    "grpComp = cmp.GraphComp()\n",
    "grp = gam.Graph_Amazon()\n",
    "\n",
    "# here, there is no Connected or Disconnected check, so, the graph generated will most likele be a DISCONNECTED graph\n",
    "DISconnected_gr_amazon_movies = grp.Create_Graph(file_name = 'data/movies.txt', n_movies = 8, prs_out = 'dictionary')\n",
    "# here, we set n_movies = 8 but we have 10 nodes. Why is that?\n",
    "# Because 8 actually shows the number of edges, not number of the movies (BAD NAMING!)\n",
    "# So, we have 8 edges and in this 8 edges, we have 10 unique nodes. It might be like 8 users + 2 movies\n",
    "\n",
    "P_disconnected = nx.to_numpy_matrix(DISconnected_gr_amazon_movies)\n",
    "print(type(P_disconnected))\n",
    "\n",
    "print('P_disconnected:\\n{}'.format(P_disconnected))\n",
    "\n",
    "P_disconnected_norm = grpComp.Normalize_Matrix(P_disconnected, dim = 'row')\n",
    "print('\\nP_disconnected-NORMALIZED:\\n{}'.format(P_disconnected_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another simulation with a smaller matrix, it migh\n",
    "tt2 = np.array([[0, 4, 2], [4, 0, 0], [2, 0, 0]])\n",
    "tt2 = np.asmatrix(tt2)\n",
    "print(type(tt2))\n",
    "#tt2_norm = cmp1.Normalize_Matrix(tt2, dim = 'tt2')\n",
    "tt2_norm = cmp1.Normalize_Matrix(tt2, dim = 'row')\n",
    "\n",
    "print('Sample matrix:\\n{}'.format(tt2))\n",
    "print('\\nSample matrix_NORM:\\n{}'.format(tt2_norm))\n",
    "\n",
    "r_tt = np.zeros(tt2_norm.shape[0])\n",
    "r_tt[1] = 1\n",
    "r_tt_zero = r_tt.copy()\n",
    "\n",
    "print('r at the beginning:\\n{}'.format(r_tt_zero))\n",
    "\n",
    "print('\\nLets see how random walk works by selecting the second node_P[1]:'.format(r_tt))\n",
    "#r = (1 - beta) * np.dot(r, P_norm) + beta * r_zero\n",
    "\n",
    "# r vectors can be generated in the function but we started to design in that way, sos didn't change it later..\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('r - after 1st step:\\n{}'.format(r_tt))\n",
    "# as you can see, after the first strep, the value for r[1]=0.15  and the SUM(row) = 1\n",
    "\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 2nd step:\\n{}'.format(r_tt))\n",
    "\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 3rd step:\\n{}'.format(r_tt))\n",
    "\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 4th step:\\n{}'.format(r_tt))\n",
    "\n",
    "r_tt = grpComp.random_walk_vector(tt2_norm, r_tt, r_tt_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 5th step:\\n{}'.format(r_tt))\n",
    "\n",
    "# You will easily realize that, for the even number of steps, the probability is high on the user himself,\n",
    "# on the odd number of steps, the probability is high on the movie he reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpComp = cmp.GraphComp()\n",
    "\n",
    "# *** min numner of reviews is 140 \n",
    "# if we want to make a connected bipartite graph & if we start picking from review#1\n",
    "grp = gam.Graph_Amazon()\n",
    "file_name='data/movies.txt'; n_movies = 140; prs_out='dictionary'\n",
    "\n",
    "max_connected_gr_amazon_movies = grpComp.Create_Bipartite_Giant_Component(grp, file_name, n_movies, prs_out)\n",
    "\n",
    "P = nx.to_numpy_matrix(max_connected_gr_amazon_movies)\n",
    "\n",
    "grpComp.Normalize_Matrix(P, dim = 'row')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.zeros(P_disconnected_norm.shape[0])\n",
    "r[1] = 1\n",
    "r_zero = r.copy()\n",
    "\n",
    "print('r at the beginning:\\n{}'.format(r_zero))\n",
    "\n",
    "print('\\nLets see how random walk works by selecting the second node_P[1]:'.format(r))\n",
    "#r = (1 - beta) * np.dot(r, P_norm) + beta * r_zero\n",
    "\n",
    "r = grpComp.random_walk_vector(P_disconnected_norm, r, r_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 1st step:\\n{}'.format(r))\n",
    "# as you can see, after the first strep, the value for r[1]=0.15  and the SUM(row) = 1\n",
    "\n",
    "r = grpComp.random_walk_vector(P_disconnected_norm, r, r_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 2nd step:\\n{}'.format(r))\n",
    "\n",
    "r = grpComp.random_walk_vector(P_disconnected_norm, r, r_zero, beta=0.15, n_steps=1)\n",
    "print('\\nr - after 3rd step:\\n{}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Common items in 2 lists with SET (FASTER) - WITHOUT FOR loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_nodes = ['u1', 'u2', 'u3', 'u4']\n",
    "bottom_nodes_v = ['u_v1', 'u2', 'u_v3', 'u4', 'u_v5']\n",
    "lst_common_users = list(set(bottom_nodes) - (set(bottom_nodes) - set(bottom_nodes_v)))\n",
    "#lst_common_users = list(set(bottom_nodes_v) - (set(bottom_nodes_v) - set(bottom_nodes)))\n",
    "print(lst_common_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph from LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test = [('A141HP4LYPW', 'B003AI2VGA', '3.0'),\n",
    "     ('B000063W1R', 'A141HP4LYPW', '4.0'),\n",
    "     ('6304286961', 'A141HP4LYPW', '5.0'),\n",
    "     ('A141HP4LYPW', '5556167281', '5.0')]\n",
    "\n",
    "grp1 = gam.Graph_Amazon()\n",
    "grp_test=grp1.Create_Graph_From_List_WITH_Weight(lst_test)\n",
    "\n",
    "print(grp_test.nodes)\n",
    "\n",
    "grp_test_martrix = nx.to_numpy_matrix(grp_test)\n",
    "\n",
    "print(grp_test_martrix)\n",
    "# as uyou can see, we have 1 user 4 movies in this set (which is unusual, movies are usually less)\n",
    "# but this is just an artificial set - you can easily play and test some functions if you would like to now\n",
    "# i.e. Normalization, Random walk, check networkx functions, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_test.edges.data('weight', default=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_test.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(grp_test.nodes)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_test.has_edge('A141HP4LYPW', 'B003AI2VGAx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdata = grp_test.get_edge_data('A141HP4LYPW', 'B003AI2VGA')\n",
    "print(gdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdata['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_test.get_edge_data('A141HP4LYPW', 'B003AI2VGA')['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grp_test.get_edge_data('A141HP4LYPW', 'B003AI2VGAx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WAS AN INTERNAL TEST - just keeping as a side note..\n",
    "# by doing the below conversion, we are just considering the links between the nodes, we ignore the weights\n",
    "# we try to understand if considering the weights will benefit us or not\n",
    "#P[P>0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(llt)[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nodes=np.argsort(r)\n",
    "\n",
    "user_similarity = []\n",
    "\n",
    "# we will not consider the user himself or movies as similar nodes\n",
    "for idx in sorted_nodes[0][0]:\n",
    "    if idx != 3 and idx > 0:\n",
    "        user_similarity.append(idx)\n",
    "        \n",
    "user_similarity_top2 = user_similarity[-2:]\n",
    "\n",
    "print(user_similarity)\n",
    "print(user_similarity_top2)\n",
    "print(list(grp_test.nodes)[user_similarity_top2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Edges from movies file, OUTPUT: Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code was quickly written so, it might not be necessarily starting from the 4th review but maybe 5th review\n",
    "# sensitive calibration haven't done, so index can be +1. \n",
    "# but in any case, it gurantees to avoid the overlap (if unlucky, max 1 overlap will be in place)\n",
    "file_name = 'data/movies.txt'; start_index = 4; n_movies = 5; prs_out = 'screen'\n",
    "\n",
    "#movie_dict_old = amp.AmazonMovies(n_movies, file_name, prs_out)\n",
    "movie_dict_old = amp.AmazonMovies_VALIDATION(n_movies, file_name, prs_out, start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = 'data/movies.txt'; start_index = 2; n_movies = 10; prs_out = 'screen'\n",
    "\n",
    "movie_dict = amp.AmazonMovies_VALIDATION(n_movies, file_name, prs_out, start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = gam.Graph_Amazon()\n",
    "grp_test = grp.Create_Bipartite_VALIDATION(file_name = 'data/movies.txt', start_index = 0, \n",
    "                            n_movies = 12000, prs_out = 'dictionary', file_type = 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_part1 = datetime.datetime.now()\n",
    "diameter_of_graph = nx.diameter(grp_test)\n",
    "end_time_part1 = datetime.datetime.now()\n",
    "calc_time=end_time_part1-start_time_part1\n",
    "print(diameter_of_graph)\n",
    "print('calc time= {}'.format(calc_time))\n",
    "# diameter for graphs with the edges:\n",
    "# 12K = 13 .... takes time: 13:24 minutes\n",
    "# 20K = 13 .... takes time: 35:45 minutes (35 minutes, 45 seconds)\n",
    "# We will not calculate anymore since it looks like it is independent of the size \n",
    "# of the graph and in average, it ends up as 13. Moreover, can we use this diameter \n",
    "# function on a bipartite graph is another question***\n",
    "# 30K =  .... takes time: \n",
    "# 40K =  .... takes time:  \n",
    "# 50K =  .... takes time:  \n",
    "# 55K =  .... takes time: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH in movies.txt for specific items (FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=1274406 .. ts=2012-09-11 .. num_rec=141600\n",
      "i=1925250 .. ts=2004-12-19 .. num_rec=177757\n",
      "i=9206322 .. ts=2012-09-11 .. num_rec=582260\n",
      "i=9863772 .. ts=2012-09-11 .. num_rec=618784\n",
      "i=18642876 .. ts=2009-09-16 .. num_rec=1106511\n",
      "i=19191660 .. ts=2009-09-16 .. num_rec=1136998\n",
      "i=25368990 .. ts=2004-07-21 .. num_rec=1480182\n",
      "i=25472220 .. ts=2004-12-19 .. num_rec=1485916\n",
      "i=26664306 .. ts=2008-06-02 .. num_rec=1552142\n",
      "i=29832738 .. ts=2012-10-18 .. num_rec=1728165\n",
      "i=31718418 .. ts=2009-09-16 .. num_rec=1832924\n",
      "i=32248176 .. ts=2012-09-11 .. num_rec=1862354\n",
      "i=34394424 .. ts=2012-09-13 .. num_rec=1981589\n",
      "i=38462460 .. ts=2010-09-01 .. num_rec=2207590\n",
      "i=38720346 .. ts=2012-09-11 .. num_rec=2221916\n",
      "i=39821136 .. ts=2004-12-19 .. num_rec=2283070\n",
      "i=48661800 .. ts=2004-09-08 .. num_rec=2774217\n",
      "i=48661809 .. ts=2004-09-08 .. num_rec=2774217\n",
      "i=52843524 .. ts=2007-09-05 .. num_rec=3006534\n",
      "i=53280654 .. ts=2000-12-13 .. num_rec=3030818\n",
      "i=54416256 .. ts=2012-09-11 .. num_rec=3093906\n",
      "i=55813722 .. ts=2009-09-16 .. num_rec=3171542\n",
      "i=56866524 .. ts=2012-09-11 .. num_rec=3230030\n",
      "i=59931888 .. ts=2004-09-29 .. num_rec=3400327\n",
      "i=62980206 .. ts=2009-09-16 .. num_rec=3569677\n",
      "i=63946554 .. ts=2012-09-11 .. num_rec=3623362\n"
     ]
    }
   ],
   "source": [
    "import Amazon_Movie_Parser as amp\n",
    "import datetime\n",
    "start_time_part1 = datetime.datetime.now()\n",
    "file_name = 'data/movies.txt'; n_movies=500000; prs_out = 'dictionary'; num_limit=5000000; date_year=2009\n",
    "\n",
    "#userID = '';   movieID = ''  #movieID = 'B00005952Q'\n",
    "#userID = 'AR15V2ULA2EYM'   #1182729600\n",
    "#pd.set_option('display.max_colwidth', 2000)\n",
    "#tst_dict = amp.Filter_AmazonMovies(n_movies, userID, movieID, file_name, outputTo='dictionary')\n",
    "tst_dict = amp.AmazonMovies(num_limit, n_movies, date_year, file_name, prs_out)\n",
    "\n",
    "end_time_part1 = datetime.datetime.now()\n",
    "calc_time=end_time_part1-start_time_part1\n",
    "\n",
    "print('calc time= {}'.format(calc_time))\n",
    "#amp.Filter_AmazonMovies_2(n_movies, userID, movieID, file_name, prs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lst_movie = []\n",
    "lst_user = []\n",
    "lst_rating = []\n",
    "lst_time = []    \n",
    "            \n",
    "for review in tst_dict.values():\n",
    "    #ts = int(review[2])\n",
    "    #ts_datetime = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    #ts_datetime = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d')\n",
    "\n",
    "    lst_movie.append(review[0])\n",
    "    lst_user.append(review[1])\n",
    "    lst_time.append(review[2])\n",
    "    #lst_time.append(ts_datetime)\n",
    "    lst_rating.append(review[3])\n",
    "                \n",
    "dict_reviews = {'users': lst_user, 'movies': lst_movie, 'rating': lst_rating, 'time': lst_time}\n",
    "\n",
    "df_reviews = pd.DataFrame(dict_reviews).sort_values('time')\n",
    "df_reviews = df_reviews.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.head(10)\n",
    "\n",
    "#duration = 1  # seconds\n",
    "#freq = 440  # Hz\n",
    "#os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>movies</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5625</td>\n",
       "      <td>ABLY9PHJR8LQ3</td>\n",
       "      <td>B003UMW648</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5626</td>\n",
       "      <td>A1YV7JR4Z1G600</td>\n",
       "      <td>B0009IXRIA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5627</td>\n",
       "      <td>A3IYMY6QNAVWBM</td>\n",
       "      <td>B001KNAI2Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5628</td>\n",
       "      <td>A37OOQQLMMKYR8</td>\n",
       "      <td>B00006L9XJ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2009-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5629</td>\n",
       "      <td>AT99KGZL5IZD5</td>\n",
       "      <td>B0010X73ZG</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9658</td>\n",
       "      <td>A25ZAY25GR5CFR</td>\n",
       "      <td>B000FDF78W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2009-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9659</td>\n",
       "      <td>A1LR6W5Q4U32ZW</td>\n",
       "      <td>6300183238</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2009-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9660</td>\n",
       "      <td>A181XRGXAO6COB</td>\n",
       "      <td>B000N4SHOE</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2009-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9661</td>\n",
       "      <td>A2PWMLTJ8L3IYD</td>\n",
       "      <td>B001F8YPLG</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2009-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9662</td>\n",
       "      <td>A2E5CJT2218BYZ</td>\n",
       "      <td>B001NKI60W</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009-02-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4038 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               users      movies rating        time\n",
       "5625   ABLY9PHJR8LQ3  B003UMW648    5.0  2009-02-01\n",
       "5626  A1YV7JR4Z1G600  B0009IXRIA    5.0  2009-02-01\n",
       "5627  A3IYMY6QNAVWBM  B001KNAI2Q    5.0  2009-02-01\n",
       "5628  A37OOQQLMMKYR8  B00006L9XJ    4.0  2009-02-01\n",
       "5629   AT99KGZL5IZD5  B0010X73ZG    5.0  2009-02-01\n",
       "...              ...         ...    ...         ...\n",
       "9658  A25ZAY25GR5CFR  B000FDF78W    4.0  2009-02-28\n",
       "9659  A1LR6W5Q4U32ZW  6300183238    3.0  2009-02-28\n",
       "9660  A181XRGXAO6COB  B000N4SHOE    4.0  2009-02-28\n",
       "9661  A2PWMLTJ8L3IYD  B001F8YPLG    2.0  2009-02-28\n",
       "9662  A2E5CJT2218BYZ  B001NKI60W    5.0  2009-02-28\n",
       "\n",
       "[4038 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.query(\"time < '2009-03-01' and time >= '2009-02-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('say \"your program has finished\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str_date = \"2012-10-25\"\n",
    "\n",
    "int_date=1182729600\n",
    "dt_date=datetime.utcfromtimestamp(int_date).strftime('%Y-%m-%d')\n",
    "#ts = datetime.date(*(int(s) for s in dt_date.split('-')))\n",
    "#ts\n",
    "dt_date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Amazon_Movie_Parser as amp\n",
    "import datetime\n",
    "# there is a problem with the format at i=1274406 .. ts=2012-09-11 .. num_rec=141600\n",
    "start_date=datetime.datetime.now()\n",
    "file_name = 'data/movies.txt'; num_limit=141600; n_movies=50000; prs_out = 'dictionary'; date_year=2009\n",
    "dict_test=amp.AmazonMovies_DATE(num_limit, n_movies, date_year, file_name, prs_out)\n",
    "end_date=datetime.datetime.now()\n",
    "print('Calculartion time: {}'.format(end_date-start_date))\n",
    "dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "x=i+9-i%9+1\n",
    "x%9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Amazon_Movie_Parser as amp\n",
    "from datetime import datetime\n",
    "file_name = 'data/movies.txt'; num_limit=20; n_movies=50000; prs_out = 'screen'; date_year=2009\n",
    "amp.AmazonMovies_VALIDATION_DEBUG(num_limit, file_name, prs_out, start_index = 141605)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12743146%9\n",
    "12743137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the search uses 'like' criteria so it search for any string mathcing \n",
    "# not a perfect search, just used to debug something -can definitely be improved..\n",
    "import Amazon_Movie_Parser as amp\n",
    "from datetime import datetime\n",
    "file_name = 'data/movies.txt'; n_movies=100; prs_out = 'screen'\n",
    "userID = '';   movieID = ''  #movieID = 'B00005952Q'\n",
    "#userID = 'AR15V2ULA2EYM'   #1182729600\n",
    "#pd.set_option('display.max_colwidth', 2000)\n",
    "#tst_dict = amp.Filter_AmazonMovies(n_movies, userID, movieID, file_name, outputTo='dictionary')\n",
    "review_dict = amp.AmazonMovies(n_movies, file_name, outputTo='dictionary')\n",
    "list(review_dict.values())\n",
    "\n",
    "lst_movie = []\n",
    "lst_user = []\n",
    "lst_rating = []\n",
    "lst_time = []\n",
    "\n",
    "for review in review_dict.values():\n",
    "    ts = int(review[2])\n",
    "    #ts_datetime = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ts_datetime = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d')\n",
    "    \n",
    "    lst_movie.append(review[0])\n",
    "    lst_user.append(review[1])\n",
    "    #lst_time.append(review[2])\n",
    "    lst_time.append(ts_datetime)\n",
    "    lst_rating.append(review[3])\n",
    "    #print('ts={} ts_datetime={}'.format(ts, ts_datetime))\n",
    "    #print(review)\n",
    "    \n",
    "dict_reviews = {'users': lst_user, 'movies': lst_movie, 'rating': lst_rating, 'time': lst_time}\n",
    "\n",
    "df_reviews = pd.DataFrame(dict_reviews).sort_values('time')#, inplace=True\n",
    "print(df_reviews)\n",
    "\n",
    "#ts = 1012003200\n",
    "#ts_datetime = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#print(ts_datetime)\n",
    "# 2000-07-30 00:00:00  ... 964915200\n",
    "# 2002-01-26 00:00:00  ... 1012003200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_filtered = df_reviews.query(\"time > '2012-07-30'\")\n",
    "# if you want to re-index the indexes\n",
    "df_time_filtered=df_time_filtered.reset_index(drop=True)\n",
    "#print(df_time_filtered[df_time_filtered.index >= 2 and df_time_filtered.index < 4])\n",
    "#print(df_time_filtered.query(\"index >= 2 and index <4\"))\n",
    "n_movies=3\n",
    "n_movies_val=2\n",
    "df_reviews_model = df_time_filtered.query(\"index < \" + str(n_movies))\n",
    "print(df_reviews_model)\n",
    "df_reviews_val = df_time_filtered.query(\"index >= \" + str(n_movies) + \" and index < \" + str(n_movies + n_movies_val))\n",
    "print(df_reviews_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in df_time_filtered.values:\n",
    "    print(r[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_filtered.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "ts = int(\"1037491200\")\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_true = [0, 0, 0, 0, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 0, 1]\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "lst_tpr = list(tpr); lst_fpr = list(fpr); lst_thresholds = list(thresholds)\n",
    "\n",
    "print(fpr)\n",
    "print(tpr)\n",
    "print(thresholds)\n",
    "\n",
    "len_tpr = len(lst_tpr)\n",
    "lst_precision = np.zeros(len_tpr)\n",
    "lst_recall = np.zeros(len_tpr)\n",
    "        \n",
    "for i in range(len(lst_tpr)):\n",
    "    lst_precision[i] = lst_tpr[i]/(lst_tpr[i] + lst_fpr[i])\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(roc_auc)\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average='macro') # 'micro'  'weighted' None  average=None, zero_division=1\n",
    "print(precision)\n",
    "\n",
    "precision_recall = precision_recall_curve(y_true, y_pred)\n",
    "print(list(precision_recall))\n",
    "\n",
    "avg_precision = average_precision_score(y_true, y_pred)\n",
    "print(avg_precision)\n",
    "\n",
    "print(lst_sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1=[1,2,3]\n",
    "lst2=[3,5,1]\n",
    "lst3=[8,2,4]\n",
    "lstr = [lst1, lst2, lst3]\n",
    "lstr_2 = []\n",
    "\n",
    "max_i=0\n",
    "max_i_val=0\n",
    "for i, l in enumerate(lstr):\n",
    "    lstr_2.append(l[2])\n",
    "    if l[0] > max_i_val:\n",
    "        max_i_val = l[0]\n",
    "        max_i = i\n",
    "\n",
    "print(max_i)\n",
    "print(max_i_val)\n",
    "print(lstr[max_i])\n",
    "print(lstr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst4=[8,2,4]\n",
    "print(lst4)\n",
    "lst4.remove(8)\n",
    "print(lst4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Test to Check if any RUN-time Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'data/movies.txt'; n_movies = 5000; n_movies_v = 3000; n_test_users = 20; walk_steps = 2\n",
    "start_index_v = n_movies; beta = 0.15; top_neighbor = 10; top_N_movie_suggestions = 5\n",
    "\n",
    "predict_amazon = pm.Amazon()\n",
    "strt=datetime.datetime.now()\n",
    "max_connected_gr_amazon_movies, max_connected_gr_amazon_movies_VAL = \\\n",
    "    predict_amazon.create_graphs(file_name, n_movies, n_movies_v, start_index_v)\n",
    "\n",
    "df_FINAL = \\\n",
    "    predict_amazon.run_validation_TOP_N(max_connected_gr_amazon_movies, max_connected_gr_amazon_movies_VAL,\\\n",
    "                                  walk_steps, n_test_users, beta, top_neighbor, top_N_movie_suggestions)\n",
    "end1=datetime.datetime.now()\n",
    "diff=end1-strt\n",
    "df_perf = predict_amazon.save_performance_values(df_FINAL, diff, diff)\n",
    "\n",
    "params_df = predict_amazon.show_parameters_in_use()\n",
    "params_df.style.set_properties(**{'text-align': 'left'})\n",
    "\n",
    "df_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N_suggestions=10\n",
    "predict_amazon.df_summary_ratio_v.sort_values(by='ratio_similar', ascending=False).head(top_N_suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'data/movies.txt'; num_reviews = 5\n",
    "amp.AmazonMovies(num_reviews, file_name, outputTo='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_tst = amp.Read_Connected_Movies('Movies_22.5.2020_20.47.56.txt')\n",
    "print(lst_tst)\n",
    "arr_tst = lst_tst[2][0].replace('[','').replace(']','').split(sep=\"' '\")\n",
    "#print(len(lst_tst))\n",
    "print(arr_tst)\n",
    "print(arr_tst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "file_name = 'data/movies.txt'; num_reviews = 75000\n",
    "\n",
    "amp.AmazonMovies(num_reviews, file_name, outputTo='dict_to_file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = amp.Load_Pickle_File_VAL('Movies_22.5.2020_23.21.10__100_reviews.pkl', 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "out = dict(list(xx.items())[2: 7])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1=[1]\n",
    "lst_2=[2,3,1]\n",
    "lst_1+=lst_2\n",
    "print(lst_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1.count(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Results from ALL CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'RESULTS/_Precision@1/PredictionDetails_50000_20500_100_40_60_TOP1_27.5.2020_18.33.51.csv'\n",
    "perf_1 = pd.read_csv(file_name)\n",
    "perf_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "perf_1['prediction']=1\n",
    "y_true = list(perf_1['reality'])\n",
    "#y_pred = list(perf_1['ngbr_ratio'])\n",
    "y_pred = list(perf_1['prediction'])\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'RESULTS/_Precision@1/PerformanceValues_0.903_26.5.2020_15.51.9.csv'\n",
    "perf_1 = pd.read_csv(file_name)\n",
    "perf_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K Recommendations Prediction Results\n",
    "file_name = 'RESULTS/_Precision@1/PerformanceValues_0.903_26.5.2020_15.51.9.csv'\n",
    "perf_1 = pd.read_csv(file_name)\n",
    "\n",
    "list_name = ['Test_No', 'AUC_Top_K', 'Precision_Top_K', 'Recall_Top_K', 'n_movies_model', 'n_movies_validation',\n",
    "            'walk_steps', 'beta', 'top_N_neighbors', 'n_test_users', 'top_K_recommendations']\n",
    "\n",
    "test_no = 0\n",
    "AUC = round(float(perf_1[\"AUC\"][0]), 6)\n",
    "precision = perf_1.query(\"AUC=='decision_4'\")['Precision'].tolist()[0]\n",
    "recall = perf_1.query(\"AUC=='decision_4'\")['TPR'].tolist()[0]\n",
    "n_movies_model = int(perf_1[\"lst_value\"][1])\n",
    "n_movies_validation = int(perf_1[\"lst_value\"][2])\n",
    "walk_steps = int(perf_1[\"lst_value\"][3])\n",
    "beta = float(perf_1[\"lst_value\"][4])\n",
    "top_N_neighbors = int(perf_1[\"lst_value\"][5])\n",
    "n_test_users = int(perf_1[\"lst_value\"][6])\n",
    "top_K_recommendations = int(perf_1[\"lst_value\"][10])\n",
    "\n",
    "list_value = [test_no, AUC, round(precision, 6), round(recall, 6), n_movies_model, n_movies_validation, \n",
    "              top_N_neighbors, n_test_users, walk_steps, beta, top_K_recommendations]\n",
    "\n",
    "list_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.random.uniform(0, 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(perf_1[\"lst_value\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "list_file = os.listdir(\"RESULTS/_Precision@1/\")\n",
    "print(list_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Amazon_movies.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
